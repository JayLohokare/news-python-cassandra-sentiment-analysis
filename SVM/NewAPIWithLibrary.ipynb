{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'newspaper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-988cf9439c19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnewspaper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnewsapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewsapi_client\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNewsApiClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'newspaper'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "from newspaper import Article\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "import pickle as p\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MongoClient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-34f9c728df6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mvectorFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'vector.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodelFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'svm.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mclient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMongoClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mongodb://root:LCl67MkFgRqV@18.208.219.105'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m27017\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'uptick_news_database'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcollection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnews\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MongoClient' is not defined"
     ]
    }
   ],
   "source": [
    "lookUpTime = 1000 #In minutes\n",
    "debugOn = 1\n",
    "printEntities = 1\n",
    "\n",
    "#All parameters go here\n",
    "domainsFile = 'domains.csv'\n",
    "keysFile = 'newsKeys.csv'\n",
    "\n",
    "mapNewsToCoinsAndNames = 'newsQuerySheet.csv'\n",
    "mapNewsToCoin = 'searchTermsForCoin.csv'\n",
    "language = 'en'\n",
    "vectorFile = 'vector.pkl'\n",
    "modelFile = 'svm.pkl'\n",
    "client = MongoClient('mongodb://root:LCl67MkFgRqV@18.208.219.105', 27017)\n",
    "db = client['uptick_news_database']\n",
    "collection = db.news\n",
    "collection2 = db.news2\n",
    "collection3 = db.news3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(string):\n",
    "    if debugOn == 1:\n",
    "        print (str(string) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesList = []\n",
    "domainsList = []\n",
    "with open(domainsFile) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        domainsList.extend(row)\n",
    "        \n",
    "domainsCommaSeperated = ','.join(domainsList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "with open(keysFile) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        keys.append(row[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleContent(url):\n",
    "    try:\n",
    "        article = None\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelatedCoinsUsingEntity(content):\n",
    "    doc = nlp(content)\n",
    "    allOrgs = []\n",
    "    for ent in doc.ents:\n",
    "        if printEntities == 1:\n",
    "            print (str(ent.text) + \"  \"  + str(ent.label_))\n",
    "        if ent.label_ == \"ORG\":\n",
    "            allOrgs.append(ent.text.lower())\n",
    "    \n",
    "    coins = []\n",
    "    \n",
    "    allOrgs = [i.lower() for i in allOrgs]\n",
    "    \n",
    "    with open(mapNewsToCoin) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            for i in row:\n",
    "                i = i.lower().strip()\n",
    "                if i in allOrgs:\n",
    "                    coins.append(row[0])\n",
    "                    break\n",
    "    return coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelatedCoinsUsingDirectMatch(content):\n",
    "    content = content.strip().lower().split()\n",
    "    coins = []\n",
    "    \n",
    "    with open(mapNewsToCoinsAndNames) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            searchTerms = row\n",
    "            areAllTermsPresent = True\n",
    "            for searchTerm in searchTerms:\n",
    "                if searchTerm.lower().strip() not in content:\n",
    "                    areAllTermsPresent = False\n",
    "                    break\n",
    "            if areAllTermsPresent:\n",
    "                coins.append(row[0])\n",
    "    \n",
    "    return coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(content):\n",
    "    content = [content]\n",
    "    file = open(vectorFile, 'rb')\n",
    "    vect = p.load(file)\n",
    "    file.close()\n",
    "\n",
    "    file = open(modelFile, 'rb')\n",
    "    SVM = p.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    test_dtm = vect.transform(content)\n",
    "    predLabel = SVM.predict(test_dtm)\n",
    "    tags = ['Negative','Neutral','Positive']\n",
    "\n",
    "    return predLabel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making single API call\n",
    "#Dont run this\n",
    "\n",
    "# searchQuery = \"cryptocurrencies\"\n",
    "# print (searchQuery)\n",
    "\n",
    "# k = random.randint(0, len(keys)-1)\n",
    "# key = keys[k]\n",
    "# newsapi = NewsApiClient(api_key=key)\n",
    "\n",
    "# temp_articles = newsapi.get_everything(q=searchQuery,\n",
    "#                                     domains= domainsCommaSeperated,\n",
    "#                                     language=language,\n",
    "#                                     from_param=fromTime,\n",
    "#                                     to=currentTime,\n",
    "#                                     )\n",
    "# all_articles = temp_articles['articles']\n",
    "\n",
    "# for j in range(len(all_articles)):\n",
    "#     url = all_articles[j]['url']\n",
    "#     contentExtracted = getArticleContent(url).strip()\n",
    "#     contentExtracted = contentExtracted.replace(\"\\'\", \"\")\n",
    "#     contentExtracted = contentExtracted.split()\n",
    "#     contentExtracted2 = []\n",
    "#     for i in contentExtracted:\n",
    "#         if i == \"Advertisement\" or i == \"advertisement\":\n",
    "#             continue\n",
    "#         else:\n",
    "#             contentExtracted2.append(i)\n",
    "\n",
    "#     contentExtracted = \" \".join(contentExtracted2)\n",
    "\n",
    "#     content = all_articles[j]['content']\n",
    "\n",
    "#     if content != \"\":\n",
    "#         tempDict = {}\n",
    "#         tempDict['url'] = url\n",
    "#         tempDict['publishedAt'] = all_articles[j]['publishedAt']\n",
    "#         tempDict['title'] = all_articles[j]['title']\n",
    "#         tempDict['description'] = all_articles[j]['description']\n",
    "#         tempDict['author'] = all_articles[j]['author']\n",
    "#         tempDict['contentExtracted'] = contentExtracted\n",
    "#         tempDict['content'] = content\n",
    "\n",
    "#         tempDict['image'] = all_articles[j]['urlToImage']\n",
    "#         tempDict['source'] = all_articles[j]['source']\n",
    "#         tempDict['language'] = language\n",
    "\n",
    "#         tempDict['sentiment'] = getSentiment(contentExtracted)\n",
    "#         tempDict['relevance'] = 0\n",
    "\n",
    "#         relatedCoins = getRelatedCoins(contentExtracted)\n",
    "\n",
    "#         print (contentExtracted)\n",
    "#         print (\"\\n\")\n",
    "#         print (url)\n",
    "#         print (\"Related coins \" , relatedCoins)\n",
    "#         print (\"\\n\")\n",
    "\n",
    "#         for coin in relatedCoins: \n",
    "#             tempDict['coin'] = coin\n",
    "#             searchDict ={}\n",
    "#             searchDict['url'] = url\n",
    "#             searchDict['coin'] = coin\n",
    "#             collection.update_one(searchDict, {\"$set\":tempDict}, upsert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-eb48f0b18548>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Making API call per coin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcurrentTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfromTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminutes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlookUpTime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapNewsToCoinsAndNames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "with open(mapNewsToCoinsAndNames) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        all_articles = []\n",
    "        queryCoinName = \"(\" + str(row[0]).strip() + ')AND(\"' +  str(row[1]).strip() + '\")'\n",
    "        searchQuery = queryCoinName \n",
    "        \n",
    "        \n",
    "        debug (searchQuery)\n",
    "        \n",
    "        # k = random.randint(0, len(keys)-1)\n",
    "        # key = keys[k]\n",
    "        key = \"445938e7b4214f4988780151868665cc\"\n",
    "        newsapi = NewsApiClient(api_key=key)\n",
    "\n",
    "        try:\n",
    "            temp_articles = newsapi.get_everything(q=searchQuery,\n",
    "                                                domains= domainsCommaSeperated,\n",
    "                                                language=language,\n",
    "                                                from_param=fromTime,\n",
    "                                                to=currentTime,\n",
    "                                                )\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        all_articles = temp_articles['articles']\n",
    "        debug (all_articles)\n",
    "\n",
    "        for j in range(len(all_articles)):\n",
    "            url = all_articles[j]['url']\n",
    "            contentExtracted = getArticleContent(url).strip()\n",
    "            contentExtracted = contentExtracted.replace(\"\\'\", \"\")\n",
    "            contentExtracted = contentExtracted.split()\n",
    "            contentExtracted2 = []\n",
    "            for i in contentExtracted:\n",
    "                if i == \"Advertisement\" or i == \"advertisement\":\n",
    "                    continue\n",
    "                else:\n",
    "                    contentExtracted2.append(i)\n",
    "\n",
    "            contentExtracted = \" \".join(contentExtracted2)\n",
    "\n",
    "            content = all_articles[j]['content']\n",
    "\n",
    "            if content != \"\":\n",
    "                tempDict = {}\n",
    "                tempDict['url'] = url\n",
    "                tempDict['publishedAt'] = all_articles[j]['publishedAt']\n",
    "                tempDict['title'] = all_articles[j]['title']\n",
    "                tempDict['description'] = all_articles[j]['description']\n",
    "                tempDict['author'] = all_articles[j]['author']\n",
    "                tempDict['contentExtracted'] = contentExtracted\n",
    "                tempDict['content'] = content\n",
    "\n",
    "                tempDict['image'] = all_articles[j]['urlToImage']\n",
    "                tempDict['source'] = all_articles[j]['source']\n",
    "                tempDict['language'] = language\n",
    "\n",
    "                tempDict['sentiment'] = getSentiment(contentExtracted)\n",
    "                tempDict['relevance'] = 0\n",
    "\n",
    "                relatedCoinsUsingEntity = getRelatedCoinsUsingEntity(contentExtracted)\n",
    "                relatedCoinsUsingDirectMatch = getRelatedCoinsUsingDirectMatch(contentExtracted)\n",
    "                \n",
    "                debug (contentExtracted)\n",
    "                debug (url)\n",
    "                debug (\"Related coins using entity \" + str(relatedCoinsUsingEntity))\n",
    "                debug (\"Related coins using direct string match \" + str(relatedCoinsUsingDirectMatch))\n",
    "                \n",
    "                for coin in relatedCoinsUsingDirectMatch: \n",
    "                    tempDict['relatedCoin'] = coin\n",
    "                    tempDict['symbol'] = row[0].strip()\n",
    "                    tempDict['coinName'] = row[1].strip()\n",
    "                    tempDict['operator1'] = row[3].strip()\n",
    "                    tempDict['operator2'] = row[4].strip()\n",
    "                    searchDict ={}\n",
    "                    searchDict['url'] = url\n",
    "                    searchDict['relatedCoin'] = coin\n",
    "                    searchDict['coinName'] = row[0].strip()\n",
    "                    collection.update_one(searchDict, {\"$set\":tempDict}, upsert=True)\n",
    "                    \n",
    "                for coin in relatedCoinsUsingEntity: \n",
    "                    tempDict['relatedCoin'] = coin\n",
    "                    tempDict['symbol'] = row[0].strip()\n",
    "                    tempDict['coinName'] = row[1].strip()\n",
    "                    tempDict['operator1'] = row[3].strip()\n",
    "                    tempDict['operator2'] = row[4].strip()\n",
    "                    searchDict ={}\n",
    "                    searchDict['url'] = url\n",
    "                    searchDict['relatedCoin'] = coin\n",
    "                    searchDict['coinName'] = row[0].strip()\n",
    "                    collection2.update_one(searchDict, {\"$set\":tempDict}, upsert=True)\n",
    "                \n",
    "                del tempDict['relatedCoin']\n",
    "                tempDict['symbol'] = row[0].strip()\n",
    "                tempDict['coinName'] = row[1].strip()\n",
    "                tempDict['operator1'] = row[3].strip()\n",
    "                tempDict['operator2'] = row[4].strip()\n",
    "                searchDict ={}\n",
    "                searchDict['url'] = url\n",
    "                searchDict['coinName'] = coin\n",
    "                collection3.update_one(searchDict, {\"$set\":tempDict}, upsert=True)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"The race to build the best public blockchain will be won by those that would scale in line with volume. On a Sunday, a blockchain project realistically did it, though for 24 hours. Waves Platform, comprising of a digital ledger project and decentralized exchange (DEX), processed 6.1 million real-time transactions in a stress test. As it found, the network faced no disruptions or delays as the test intensified. None of the transactions on its system – undertaken by users for DEX orders, transfers, token creation, etc. – experienced any slowdown, either. The Waves blockchain, according to data provided by PYWAVES, recorded a total of 108,741 transactions. Among them, 60,933 were Mass Transfers which, per Waves blog post, are specialized transactions that can hold 100 transfer at once. “A total of 6,141,108 transfers was processed by the network, with the blockchain supporting hundreds of transactions per second at peak times,” the post claimed. The platform euphorically claimed that it was the highest number of transactions ever processed by any public blockchain. Waves NG Several blockchain projects in the crypto space are attempting to find alternatives to Bitcoin’s slow transaction confirmation periods. Ethereum was posed as a solution. But, it faced the same problem CryptoKitties – a decentralized application launched on Ethereum’s blockchain – slowed down transactions on the network. While Bitcoin has opted for third-party solutions like Lightning Network to handle the volume [temporarily], Ethereum is following a test-and-implement approach by taking in answers from its community developers. Waves, to achieve a similar goal, have implemented a tech called Waves NG that helps to scale the Waves Network by selecting miners in advance, thus minimizing latency and maximizing throughput. According to Waves’ CEO and co-founder Sasha Ivanov, the protocol’s deployment on their blockchain helped them process the record transactions. “Bitcoin processes just a few transactions per second,” he said. “Ethereum’s capacity is into double-digit tps, and a handful of other blockchains have improved on this incrementally in various ways. WAVES has implemented tech that enables a step-change in transaction volumes — not just in the lab, but in the real world, on MainNet, as these figures prove beyond doubt.” The Waves post noted that other blockchain projects had not exceeded more than 2 million transactions per day. However, a tweet from Ivanov admitted that EOS, a semi-decentralized blockchain project, had in fact executed 5 million transactions within a 24-hour period. EOS had 5.5 mil at most. — Sasha Ivanov (@sasha35625) October 23, 2018 A commentator also posted a chart from Blocktivity that suggested Waves was behind five blockchain projects concerning transaction volume. The chart later earned the “bogus” status from one of the Waves followers. Featured image from Shutterstock.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
